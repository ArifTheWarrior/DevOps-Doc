
                             DevOps Note 
                      ==========================       
important site: https://www.server-world.info/en/note?os=CentOS_7&p=haproxy&f=2
systemctl restart network 

network configuration:
======================
/etc/sysconfig/network-scripts/ifcfg-enp0s3 

update those lines:

BOOTPROTO=static
ONBOOT=yes 

Add this line:


IPADDR=192.168.100.54(use those ip which is not use anymore)
NETMASK=255.255.255.0
GATEWAY=192.168.100.1
DNS1=8.8.8.8
w


:wq! save vi file 


ssh connection use:
1.MobaXterm 
2.Putty 


vi /etc/selinux/config 

SELINUX=disabled 

Process:
-------


yum update -y 

yum install -y yum-utils 

yum install wget -y 


download docker repo:

cd /etc/yum.repos.d/

wget https://download.docker.com/linux/centos/docker-ce.repo

yum install docker-ce docker-ce-cli containerd.io docker-composeplugin -y 




systemctl start docker 


systemctl status docker 



run demo container:
--------------------

docker run hello-world 



Show container:
---------------

docker ps -a 


https://hub.docker.com/_/centos

docker pull centos:7


show images: docker images 


run container image and check : 

docker run centos:7 
docker ps -a 


start container and use:


docker start <container_id> ed2702162018

docker exec -it ed2702162018 bash 



if you have any problem restart docker:

systemctl restart docker 
systemctl enable docker 



https://hub.docker.com/r/couchbase/centos7-systemd


docker pull couchbase/centos7-systemd
docker images 
docker run couchbase/centos7-systemd
docker ps -a 
docker start <container_id> ed2702162018
docker exec -it ed2702162018 bash 



cat /etc/redhat-release


[root@localhost ~]# docker inspect <container_id> 



==========================================================

Nginx updated version of Apache web server with extra feature 
Nginx useful for load balancing 

open source proxy server--mail proxmox

Apache web server install
========================
 
upadting system:
---------------

yum update -y 


install apache server:
----------------------

yum install httpd -y 


starting the service:
--------------------

systemctl start httpd

systemctl status httpd


enable the service:
-------------------

systemctl enable httpd
systemctl restart httpd



Adding Firewall Rule:
----------------------

firewall-cmd --permanent --add-service=http
firewall-cmd --permanent --add-service=https
firewall-cmd --reload 




cd /var/www/html/

vi index.html 

Hello Devops Engineers!!

:wq!


systemctl restart httpd 

ls -la 


if you are want to create a another file as like test.html ,it is not possible
but multiple compamy host their site in Apache server..how it is possible?

Possible,

Create a different folder for every different server ,
Another process should be Virtual Host 

so 

mkdir test1 

cd test1

vi index.html 

hello world!! 

:wq!



conf.d=> uses when conf virtual Host 

==>config file store in /etc/httpd
                 /               \
          conf relate on httpd  config.d relate on virtual host .>>>>Changable 


conf.modules.d--> no need to changes



# vi /etc/httpd/conf/httpd.conf ->add listening port 8080


Install NGINX on centos7
=========================

1.web server 
2.Load Balancer 
3.Proxy server 
4.shared hosting

Proxmox=>Mail server, virtualization-type 1

yum -y update 

yum install -y epel-release

yum -y install nginx



Before please stop httpd:
=======================

systemctl stop httpd
systemctl disable httpd


Starting the service
--------------------

systemctl start nginx


Enabling the service:-
-----------------------

systemctl enable nginx


Adding Firewall Rule:
--------------------
firewall-cmd --permanent --add-service=http
firewall-cmd --permanent --add-service=https
firewall-cmd --reload 



file store:/usr/share/nginx/html/
configuration: /etc/nginx/


=====================================================================================




WHCM==> Hosting panel 




======================HA Proxy===========================

master server no need httpd
worker node==>httpd

Setting Up hostname for each server

this operation run every node included HA proxy
change the hostname:
# cat /etc/hostname
# hostnamectl set-hostname node.test.com 


this operation also run every node include HA proxy:

#hostnamectl [find host name]
Configuring the balancer server
Go to /etc/hosts and put the below lines on every node
192.168.100.90 node1.localdomain
192.168.100.91 node2.localdomain
192.168.100.92 haserver.localdomain



yum==>Yellowdog Updater, Modified”


yum info haproxy
# yum install gcc pcre-devel tar make -y 
# wget http://www.haproxy.org/download/2.0/src/haproxy-2.0.7.tar.gz 
# tar xzvf haproxy.tar.gz 
# cd haproxy-2.0.7 
# make TARGET=linux-glibc
# make install 
# mkdir -p /etc/haproxy
# mkdir -p /var/lib/haproxy
# touch /var/lib/haproxy/stats 
# ln -s /usr/local/sbin/haproxy /usr/sbin/haproxy
# cp ~/haproxy-2.0.7/examples/haproxy.init /etc/init.d/haproxy
# chmod 755 /etc/init.d/haproxy
# systemctl daemon-reload 
# systemctl enable haproxy
# useradd -r haproxy ==> no need 
# haproxy -v 
# firewall-cmd --permanent --add-service=http 
# firewall-cmd --permanent --add-port=8181/tcp
# firewall-cmd --reload 
# firewall-cmd --list-all 

===========================================================

Updated:
--------


# yum info haproxy
# yum install gcc pcre-devel tar make -y
# wget http://www.haproxy.org/download/2.6/src/haproxy-2.6.0.tar.gz
# tar xzvf haproxy-2.6.0.tar.gz
# cd haproxy-2.6.0
# make TARGET=linux-glibc
# make install
# mkdir -p /etc/haproxy
# mkdir -p /var/lib/haproxy
# touch /var/lib/haproxy/stats
# ln -s /usr/local/sbin/haproxy /usr/sbin/haproxy
# cp ~/haproxy-2.6.0/examples/haproxy.init /etc/init.d/haproxy
# chmod 755 /etc/init.d/haproxy
# systemctl daemon-reload
# systemctl enable haproxy
# useradd -r haproxy
# haproxy -v
# firewall-cmd --permanent --add-service=http
# firewall-cmd --permanent --add-port=80/tcp
# firewall-cmd --reload
# firewall-cmd --list-all

Files and directories
=====================
Config File & Directory
✓ /etc/haproxy
Configuration directory
✓ /etc/haproxy/haproxy.cfg
Configuration file
✓ /var/log/ haproxy.log
Log file


HA Proxy Configuration
======================
View the Conf file from /etc/haproxy/haproxy.cfg
Take backup of this file
# cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bkp
Now Configure as below
# vi /etc/haproxy/haproxy.cfg, put this line

global 
    log /dev/log local0
    log /dev/log local1 notice 
    chroot /var/lib/haproxy
    stats timeout 30s
    user haproxy
    group haproxy
    daemon 

defaults 
    log global 
    mode http
    option httplog
    option dontlognull
    timeout connect 5000
    timeout client 50000
    timeout server 50000

frontend http_front   
   bind *:80
    stats uri /haproxy?stats
    default_backend http_back 

backend http_back 
    balance roundrobin
    server webserver_1 192.168.29.130:80 check 
    server webserver_2 192.168.29.131:80 check 



then search on browser http://192.168.29.132:80(haserver)



roundrobin:
----------- 
1 2 3 1 2 3

Leastconn:
----------
the lowest number of connections is chosen  

first: 
-------

Suppose you have a load balancer with three backend servers: Server A, Server B, and Server C. 
Each server has a maximum connection limit of 10.

When a client makes a request to the load balancer, 
the load balancer checks which backend server has an available connection slot.
 It starts with Server A, which has a numeric identifier of 1 since it is the first
 server in the farm. If Server A has fewer than 10 connections, it receives the request
 and establishes a connection with the client.

If Server A has already reached its maximum connection limit, the load balancer moves on to
 the next server with the next highest numeric identifier, which is Server B with a numeric 
identifier of 2. If Server B has available connection slots, it receives the request and establishes a connection with the client.

If Server B has also reached its maximum connection limit, the load balancer moves on to the 
next server with the next highest numeric identifier, which is Server C with a numeric identifier of 3.

 If Server C has available connection slots, it receives the request and establishes a connection with the client.

If all three servers have reached their maximum connection limit, the load balancer cannot establish a connection with the client and may return an error message or redirect the client to another server.

So, in summary, the load balancer selects the server with the lowest numeric identifier first and checks if it has available connection slots. If the server has available slots, it receives the request. If not, the load balancer moves on to the next server with the next highest numeric identifier until it finds a server with available connection slots.


Source IP:
---------

Suppose you have a load balancer with three backend servers: Server A, Server B, and Server C.
 Each server has a weight of 1, meaning they are all equally weighted.

When a client makes a request to the load balancer, the load balancer takes the source IP
 
address of the client and hashes it. This hashed value is then divided by the total weight of the running servers, which in this case is 3 (1+1+1=3). The resulting number is used to determine which server will receive the request.

For example, let's say the client's source IP address hashes to a value of 9. The load
 
balancer then divides 9 by the total weight of the running servers, which is 3. The resulting value is 3, 

which means the request will be sent to Server C, since it has a numeric identifier of 3.

Now, let's say the same client makes another request later on. Since the client's source IP
 
address is the same, it will hash to the same value of 9. This value is divided by the total
 
weight of the running servers, which is still 3. The resulting value is still 3, so the 
request will once again be sent to Server C.

This ensures that the same client IP address will always reach the same server while the 
servers stay the same. Even if one of the servers goes down or is added later on with a different weight, the load balancer will still distribute requests in a way that ensures that the same client IP address always reaches the same server.

